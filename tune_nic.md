#### NIC Ring Buffers - число буферов для передачи и приёма.

Кольцевые буферы, совместно используются драйвером устройства и сетевой картой. TX – есть передача данных, а RX – получение данных в кольцевом буфере. Как следует из названия, переполнение буфера просто перезаписывает существующие данные. Есть два способа переместить данные от сетевой карты до ядра: аппаратные прерывания и программные прерывания, названные SoftIRQs.

Кольцевой буфер RX используется, чтобы сохранить входящие пакеты, пока они не могут быть обработаны драйвером устройства. Драйвер устройства опустошает буфер RX, обычно через SoftIRQs, который помещает входящие пакеты в структуру данных ядра, названную sk_buff или «skb», чтобы начать свой путь через ядро и до приложения, которому принадлежит соответствующий сокет. Кольцевой буфер TX используется для хранения исходящих пакетов, которые предназначенные для отправки по проводам.

Эти кольцевые буферы находятся у основания стека и являются критическим моментом, в который может произойти удаление (drop) пакетов, что негативно влияет на производительность сети.

```
ethtool -g eth0

Ring parameters for eth0:
Pre-set maximums:
RX:		4096
RX Mini:	0
RX Jumbo:	0
TX:		4096
Current hardware settings:
RX:		256
RX Mini:	0
RX Jumbo:	0
TX:		256
```
В ситуации когда трафик создается маленькими пакетами то требуется увеличить размеры буферов rx и tx:
```
ethtool -G eth0 rx tx 2048
```

#### Interrupts and Interrupt Handlers

Прерывания от аппаратных средств известны как прерывания «top-half».

Сетевые карты, как правило, работают с кольцевыми буферами (DMA ring buffer) организованными в памяти, разделяемой с процессором. Каждый входящий пакет размещается в следующем доступном буфере кольца. (DMA - Direct Memory Access (Прямой доступ к памяти) — режим обмена данными между устройствами или же между устройством и основной памятью, в котором центральный процессор (ЦП) не участвует). После этого требуется сообщить системе о появлении нового пакета и передать данные дальше, в специально выделенный буфер (Linux выделяет такие буферы для каждого пакета). Для этой цели в Linux используется механизм прерываний: прерывание генерируется всякий раз, когда новый пакет поступает в систему. Чаще используется отложенные прерывания (см. в статье Linux, принципы работы с сетевой подсистемой ). В ядро Linux начиная с версии ядра 2.6 был добавлен так называемый NAPI (New API), в котором метод прерываний сочетается с методом опроса. Сначала сетевая карта работает в режиме прерываний, но как только пакет поступает на сетевой интерфейс, она регистрирует себя в poll-списке и отключает прерывания. Система периодически проверяет список на наличие новых устройств и забирает пакеты для дальнейшей обработки. Как только пакеты обработаны, карта будет удалена из списка, а прерывания включатся снова.

Жесткие прерывания можно увидеть в /proc/interrupts, где у каждой очереди есть vector прерывания в 1-м столбце. Каждой очереди RX и TX присвоен уникальный vector, который сообщает обработчику прерываний, относительно какого NIC/queue пришло прерывание. Столбцы представляют количество входящих прерываний:

```
egrep "CPU0|eth0" /proc/interrupts

              CPU0       CPU1       CPU2       CPU3
...
 45:          0          0          0          0   PCI-MSI-edge      eth0
 46:          0          0          0          0   PCI-MSI-edge      eth0-rx-0
 47:          0          0          0          0   PCI-MSI-edge      eth0-rx-1
 48:          0          0          0          0   PCI-MSI-edge      eth0-rx-2
 49:          0          0          0          0   PCI-MSI-edge      eth0-rx-3
 50:          0          0          0          0   PCI-MSI-edge      eth0-tx-0
 51:          0          0          0          0   PCI-MSI-edge      eth0-tx-1
 52:          0          0          0          0   PCI-MSI-edge      eth0-tx-2
 53:          0          0          0          0   PCI-MSI-edge      eth0-tx-3
```

- Первый столбец — номер прерывания
- CPU0 .. CPUx — счетчик обработанных прерываний
- PCI-MSI-edge — тип прерывания
- Последний столбец — название устройства

Чтобы назначить прерывание на определённое ядро необходимо прописать его номер в файле `/proc/irq/[Номер IRQ]/smp_affinity`:
```
echo [значение smp_affinity] >/proc/irq/[номер IRQ]/smp_affinity
```

#### Отложенные прерывания (softirq) 

Известны как прерывания «bottom-half», запросы программного прерывания (SoftIRQs), являются подпрограммами ядра, которые планируется запустить в то время, когда другие задачи не должны быть прерваны. Цель SoftIRQ состоит в извлечении данных из кольцевых буферов. Эти подпрограммы, выполненные в форме процессов ksoftirqd/cpu-number и, вызывают специфичные для драйвера функции кода.

После перемещения данных от драйвера к ядру, трафик двигатется вверх к сокету приложения.

SoftIRQs можно контролировать следующим образом. Каждый столбец есть ЦП:

````
watch -n1 grep RX /proc/softirq
```
Обработчик аппаратного прерывания запрещает прерывания, выполняет необходимые действия и затем разрешает прерывания. Действия, выполняемые обработчиком, должны занимать как можно меньше процессорного времени. Например, обработчик аппаратного прерывания, являющийся частью драйвера сетевой платы сохраняет пришедший по сети пакет в буфере и завершает свою работу. Всю остальную работу по обработке сетевого пакета, берет на себя программное прерывание. 

#### Networking Tools
- netstat

  Команда netstat умеет показывать сетевые соединения (входящие/исходящие), таблицу маршрутизации, статистику по сетевым    интерфейсам и т.д.
  Она извлекает информацию о сетевой подсистеме из /proc/net/ файловой системы. Эти файлы включают в себя:
```
    /etc/services -- файл трансляции служб
    /proc -- Точка монтирования файловой системы proc, которая предоставляет доступ к информации о состоянии ядра через следующие файлы.
    /proc/net/dev -- информация об устройствах
    /proc/net/raw -- информация о необрабатываемых (raw) сокетах
    /proc/net/tcp -- информация о сокетах TCP
    /proc/net/udp -- информация о сокетах UDP
    /proc/net/igmp -- информация о мультикаст IGMP
    /proc/net/unix -- информация о сокетах домена Unix
    /proc/net/ipx -- информация о сокетах IPX
    /proc/net/ax25 -- информация о сокетах AX25
    /proc/net/appletalk -- информация о сокетах DDP (appletalk)
    /proc/net/nr -- информация о сокетах NET/ROM
    /proc/net/route -- информация об IP-маршрутизации
    /proc/net/ax25_route -- информация об AX25-маршрутизации
    /proc/net/ipx_route -- информация об IPX-маршрутизации
    /proc/net/nr_nodes -- список узлов NET/ROM
    /proc/net/nr_neigh -- соседи NET/ROM
    /proc/net/ip_masquerade -- NAT-соединения
    /proc/net/snmp -- статистика
 
```
- dropwatch

  Мониторинг операций отбрасывания сетевых пакетов данных на уровне ядра ОС

- ip

  Позволяет выполнять настройку сетевой подсистемы и является заменой таких утилит, как ifconfig, route, arp.
  
- ethtool
  
  Отображает или позволяет изменить настройки сетевой карты  

- /proc/net/snmp

  Файл, который отображает данные ASCII, необходимые для IP, ICMP, TCP, UDP и управления информацией базы для snmp агента. Он также отображает в режиме реального времени статистические данные UDP-lite
  
- sysctl
  Rоманда, предназначенная для управления параметрами ядра на лету. Позволяет читать и изменять параметры ядра. Например - такие параметры как размер сегмента разделяемой памяти, ограничение на число запущенных процессов, а также включать функции наподобие маршрутизации.
```
sysctl net.ipv4.tcp_sack
net.ipv4.tcp_sack = 1
```

#### Определение узких мест в сети

Отбрасывание пакетов и переполнени границ (packet drops и overruns) обычно происходит, когда буфер RX сетевой карты не может достаточно быстро опустошиться ядром. Когда скорость, с которой данные поступают из сети превышает скорость, с которой ядро забирает на обработку пакеты, сетевая карта начинает отбрасывать входящие пакеты, т.к. буфер NIC (сетевой карты) полон, и увеличивает счетчик удаления.Соответствующий счетчик можно увидеть в ethtool статистике. Основные критерии здесь - прерывания и SoftIRQs.

Пакеты теряются, когда переполняется RX буффер. Для того чтобы смотреть статистику используйте `ethtool` поле `rx_*_errors`
```
ethtool -S eth1
```
Существуют различные инструменты, доступные для поиска проблемной области. Следует исследовать:

1. Уровень встроенного ПО адаптера
  - Следим за статистикой ethtool -S ethX
  
2. Уровень драйвера адаптера

3. Ядро Linux, IRQs или SoftIRQs
  - Проверяем /proc/interrupts и /proc/net/softnet_stat
  
4. Уровни протокола IP, TCP, UDP
  - Используем netstat -s и смотрим счетчики ошибок

Примеры узких мест

- неверное распредление прерываний IRQs

проверь /proc/interrupts и убедитесь, что прерывания распределены между несколькими ядрами центрального процессора.
```
egrep “CPU0|eth2” /proc/interrupts 
         CPU0  CPU1  CPU2  CPU3  CPU4    CPU5
 105: 1430000     0     0     0     0       0  IR-PCI-MSI-edge   eth2-rx-0
 106: 1200000     0     0     0     0       0  IR-PCI-MSI-edge   eth2-rx-1
 107: 1399999     0     0     0     0       0  IR-PCI-MSI-edge   eth2-rx-2
 108: 1350000     0     0     0     0       0  IR-PCI-MSI-edge   eth2-rx-3
 109:   80000     0     0     0     0       0  IR-PCI-MSI-edge   eth2-t
```
- Смотри если любой столбей кроме 1-го в /proc/net/softnet_statare увеличивается.
```
cat /proc/net/softnet_stat

0073d76b 00000000 000049ae 00000000 00000000 00000000 00000000 00000000 00000000 00000000 
000000d2 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 
0000015c 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000

```
- SoftIRQs недостаточно процессорного времени для опроса адаптера. Используй `sar`, `mpstat` или `top`. Чтобы определить, что отнимает много времени процессора.

- Используй `ethtool -S ethX` для опреденения ошибок адаптера
```
ethtool -S eth3
rx_over_errors: 399
rx_fifo_errors: 399
rx_missed_errors: 39
```

- ошибки приема UDP
```
netstat -su
IcmpMsg:
    InType0: 218
    InType3: 16704
    InType5: 7
    InType8: 2
    OutType0: 2
    OutType3: 3218
    OutType8: 304
Udp:
    1067183 packets received
    2981 packets to unknown port received.
    18 packet receive errors
    992424 packets sent
    InCsumErrors: 18
    IgnoredMulti: 1530260
```

- Использование нескольких потоков TCP. 
Использование большего числа потоков TCP является более эффективным. Для просмотра потоков, которые использует приложение:
```
netstat -neopa
```

- Использование больших размеров TCP или UDP пакетов.
Каждый сетевой пакет имеет overhead, такие как загаловки. Отправка данных в большими непрерывными блоками позволит снизить этот overhead.

- Могут быть изменения работы драйвера сетевого интрефейса, после перехода на новое ядро версия Red Hat Enterprise Linux.

#### Настройки производительности

Если выполнение программных прерываний не выполняются достаточно долго, то темп роста входящих данных может превысить возможность ядра опустошить буфер. В результате буферы NIC переполнятся, и трафик будет потерян. Иногда, необходимо увеличить длительность работы SoftIRQs (программных прерываний) с CPU. За это отвечает netdev_budget. Значение по умолчанию 300. Параметр заставит процесс SoftIRQ обработать 300 пакетов от NIC перед тем как отпустить CPU:

```
# sysctl net.core.netdev_budget
net.core.netdev_budget = 300
```
Это значение может быть удвоено, если 3-й столбец в /proc/net/softnet_stat увеличивается, что указывает, на то, что SoftIRQ не получил достаточно процессорного времени. Маленькие инкременты нормальны и не требуют настройки.

